[Unit]
Description=Inference of LLaMA model in pure C/C++
Documentation=https://github.com/ggerganov/llama.cpp/tree/master/examples/server
After=network.target
Wants=network-online.target

[Service]
EnvironmentFile=-/etc/default/%p
Type=simple
User=llamaup
PermissionsStartOnly=true
WorkingDirectory=/opt/llamaup/app
ExecStart=/opt/llamaup/app/llama.cpp/build/bin/server --port 8080 --host 127.0.0.1 -m  /opt/llamaup/data/model.gguf __LLAMACPP_SERVER_EXTRA_ARGS__
Restart=on-failure

[Install]
WantedBy=multi-user.target
Alias=llamacpp.service